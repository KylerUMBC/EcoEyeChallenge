{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c75155",
   "metadata": {},
   "source": [
    "# Booz Allen Spring 2025 Codefest: Challenge 1 \n",
    "### Isaiah Byrd, Kyler Gelissen, David Ameh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf53e88a-a531-4ef9-8f93-abc1a0bfc7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyler\\.cache\\kagglehub\\datasets\\sumn2u\\garbage-classification-v2\\versions\\8\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sumn2u/garbage-classification-v2\")\n",
    "print(path)\n",
    "\n",
    "dataset_root = os.path.join(path, \"garbage-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17afb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory garbage_classification_dataset already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to store the dataset\n",
    "dataset_dir = \"garbage_classification_dataset\"\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "    print(f\"Directory {dataset_dir} created.\")\n",
    "else:\n",
    "    pass\n",
    "    print(f\"Directory {dataset_dir} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "596913e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Function to install a package\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"matplotlib\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"opencv-python\",\n",
    "    \"kagglehub\"\n",
    "]\n",
    "\n",
    "# Install each package if not already installed\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        install(package)\n",
    "\n",
    "# Importing the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import json\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd0698b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "#device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "#print(f\"Using device: {device}\")\n",
    "\n",
    "#Change the memory fraction to limit the GPU memory usage\n",
    "#Set the memory fraction of the total GPU memory\n",
    "memory_fraction = 0.80\n",
    "\n",
    "if(device.type == \"cuda\"):\n",
    "    print(\"GPU is available\")\n",
    "    th.cuda.set_memory_fraction(memory_fraction, device=device.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f23c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the data transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), #Image needs to be resized to 224x224 for ResNet requirement\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #Standard normalization for ResNet\n",
    "])\n",
    "\n",
    "#Load the dataset\n",
    "complete_dataset = datasets.ImageFolder(root=dataset_root, transform=data_transforms) \n",
    "dataset_size = len(complete_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a8d932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 19762\n",
      "Train size: 15809\n",
      "Validation size: 1976\n",
      "Test size: 1977\n"
     ]
    }
   ],
   "source": [
    "#We are going to do a 80-10-10 split of the dataset to start\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "print(f\"Dataset size: {dataset_size}\")\n",
    "print(f\"Train size: {train_size}\")\n",
    "print(f\"Validation size: {val_size}\")\n",
    "print(f\"Test size: {test_size}\")\n",
    "\n",
    "#Using random_split to split the dataset into train, validation and test sets\n",
    "train_dataset, val_dataset, test_dataset = th.utils.data.random_split(complete_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde1f386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names from dataset: ['battery', 'biological', 'cardboard', 'clothes', 'glass', 'metal', 'paper', 'plastic', 'shoes', 'trash']\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "#Creating the dataloaders\n",
    "batch_size = 64 #Update this if memory permits\n",
    "num_workers = 4 #Decrease if colab is mad\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True) \n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "#Fixed the dataset classes so it doesn't need to be manually set\n",
    "class_names = complete_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Class names from dataset: {class_names}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0653c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\Kyler/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 25.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(weights=ResNet50_Weights.DEFAULT) #Using ResNet50 as the base model for transfer learning\n",
    "\n",
    "#This takes the last layer of the model and replaces it with a new one\n",
    "num_ftrs = model.fc.in_features \n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "model = model.to(device) #Moving the model to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a51762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#Learning rate is set to 1e-4 as a starting point, update this if needed\n",
    "learning_rate = 1.e-4 \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7688f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_empochs = 2 #Update this if needed\n",
    "train_accuracy_history = []\n",
    "val_accuracy_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ddc319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main training loop\n",
    "for epoch in  range(num_empochs):\n",
    "    model.train() #Sets the model to training mode\n",
    "    running_corrects = 0\n",
    "    running_samples = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        #Move the inputs and labels to the device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() #Backpropagation step\n",
    "        optimizer.step() #Optimizer step\n",
    "\n",
    "        #Get the stupid predictions\n",
    "        preds = th.argmax(outputs, dim=1)\n",
    "        preds = preds.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        running_corrects += np.sum(preds == labels)\n",
    "        running_samples += labels.shape[0]\n",
    "    \n",
    "    #Calculate the accuracy\n",
    "    epoch_train_accuracy = running_corrects / running_samples\n",
    "    train_accuracy_history.append(epoch_train_accuracy)\n",
    "\n",
    "    #Validation step\n",
    "    model.eval() #Sets the model to evaluation mode\n",
    "    running_corrects = 0\n",
    "    running_samples = 0\n",
    "    with th.no_grad(): #Disables gradient calculation\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds = th.argmax(outputs, dim=1)\n",
    "            preds = preds.cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            running_corrects += np.sum(preds == labels)\n",
    "            running_samples += labels.shape[0]\n",
    "\n",
    "    #Calculate the accuracy\n",
    "    epoch_val_accuracy = running_corrects / running_samples\n",
    "    val_accuracy_history.append(epoch_val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_empochs} - Train accuracy: {epoch_train_accuracy:.4f} - Validation accuracy: {epoch_val_accuracy:.4f}\")\n",
    "\n",
    "train_accuracy_history = np.array(train_accuracy_history)\n",
    "val_accuracy_history = np.array(val_accuracy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac9ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the training and validation accuracy\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.arange(1, num_empochs+1), train_accuracy_history, label='Train Accuracy')\n",
    "plt.plot(np.arange(1, num_empochs+1), val_accuracy_history, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829bdce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test loop\n",
    "model.eval()\n",
    "running_corrects = 0\n",
    "running_samples = 0\n",
    "with th.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = th.argmax(outputs, dim=1)\n",
    "        running_corrects += th.sum(preds == labels).item()  #More efficient on GPU\n",
    "        running_samples += labels.size(0)\n",
    "test_acc = running_corrects / running_samples\n",
    "print(f\"Test Accuracy (full set): {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db91cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves the model\n",
    "model_path = \"garbage_classification_model.pth\"\n",
    "th.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4cb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display a batch of images\n",
    "with th.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = th.argmax(outputs, dim=1)\n",
    "        images = inputs.cpu().numpy()  #Shape:(batch_size, 3, 224, 224)\n",
    "        actual_labels = labels.cpu().numpy()\n",
    "        predicted_labels = preds.cpu().numpy()\n",
    "        break  #One batch for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to denormalize the images to display them\n",
    "def denormalize(image):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    #This rescales the image to the original range\n",
    "    image = image * std[:, None, None] + mean[:, None, None]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a7e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(images)), 6)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    image = images[idx]  #Shape: (3, 224, 224)\n",
    "    actual = class_names[actual_labels[idx]]\n",
    "    predicted = class_names[predicted_labels[idx]]\n",
    "    \n",
    "    #Denormalize and transpose image to (H, W, C) for Matplotlib\n",
    "    image = denormalize(image)\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    image = np.clip(image, 0, 1)  #Ensure pixel values are between 0 and 1\n",
    "    \n",
    "    #Display the image\n",
    "    axes[i].imshow(image)\n",
    "    \n",
    "    #Set title with actual and predicted (green if correct, red if incorrect)\n",
    "    title_color = 'green' if actual == predicted else 'red'\n",
    "    axes[i].set_title(f\"Actual: {actual}\\nPredicted: {predicted}\", \n",
    "                      color=title_color, \n",
    "                      fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "#Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
