{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c75155",
   "metadata": {},
   "source": [
    "# Booz Allen Spring 2025 Codefest: Challenge 1 \n",
    "### Isaiah Byrd, Kyler Gelissen, David Ameh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53e88a-a531-4ef9-8f93-abc1a0bfc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sumn2u/garbage-classification-v2\")\n",
    "#print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17afb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to store the dataset\n",
    "dataset_dir = \"garbage_classification_dataset\"\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "    print(f\"Directory {dataset_dir} created.\")\n",
    "else:\n",
    "    pass\n",
    "    print(f\"Directory {dataset_dir} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596913e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set divice to GPU if available\n",
    "divice = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Change the memory fraction to limit the GPU memory usage\n",
    "#Set the memory fraction to 60% of the total GPU memory\n",
    "memory_fraction = 0.6\n",
    "\n",
    "#Limits the GPU memory usage to 60% \n",
    "if(divice.type == \"cuda\"):\n",
    "    print(\"GPU is available\")\n",
    "    th.cuda.set_memory_fraction(memory_fraction, divice=divice.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f23c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the data transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), #Image needs to be resized to 224x224 for ResNet requirement\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #Standard normalization for ResNet\n",
    "])\n",
    "\n",
    "#Load the dataset\n",
    "complete_dataset = datasets.ImageFolder(root=path, transform=data_transforms) \n",
    "dataset_size = len(complete_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to do a 80-10-10 split of the dataset to start\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "print(f\"Dataset size: {dataset_size}\")\n",
    "print(f\"Train size: {train_size}\")\n",
    "print(f\"Validation size: {val_size}\")\n",
    "print(f\"Test size: {test_size}\")\n",
    "\n",
    "#Useing random_split to split the dataset into train, validation and test sets\n",
    "train_dataset, val_dataset, test_dataset = th.utils.data.random_split(complete_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dataloaders\n",
    "batch_size = 16 #Update this if memory permits\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Manually defining the class names as per the dataset\n",
    "class_names = ['Metal', 'Glass', 'Biological', 'Paper', 'Battery', 'Trash', 'Cardboard', 'Shoes', 'Clothes', 'Plastic']\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0653c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
