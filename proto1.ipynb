{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c75155",
   "metadata": {},
   "source": [
    "# Booz Allen Spring 2025 Codefest: Challenge 1 \n",
    "### Isaiah Byrd, Kyler Gelissen, David Ameh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf53e88a-a531-4ef9-8f93-abc1a0bfc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sumn2u/garbage-classification-v2\")\n",
    "#print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f17afb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory garbage_classification_dataset already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to store the dataset\n",
    "dataset_dir = \"garbage_classification_dataset\"\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "    print(f\"Directory {dataset_dir} created.\")\n",
    "else:\n",
    "    pass\n",
    "    print(f\"Directory {dataset_dir} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596913e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Function to install a package\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"matplotlib\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"opencv-python\",\n",
    "    \"kagglehub\"\n",
    "]\n",
    "\n",
    "# Install each package if not already installed\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        install(package)\n",
    "\n",
    "# Importing the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fd0698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set divice to GPU if available\n",
    "divice = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Change the memory fraction to limit the GPU memory usage\n",
    "#Set the memory fraction to 60% of the total GPU memory\n",
    "memory_fraction = 0.80\n",
    "\n",
    "#Limits the GPU memory usage to 60% \n",
    "if(divice.type == \"cuda\"):\n",
    "    print(\"GPU is available\")\n",
    "    th.cuda.set_memory_fraction(memory_fraction, divice=divice.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f23c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the data transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), #Image needs to be resized to 224x224 for ResNet requirement\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #Standard normalization for ResNet\n",
    "])\n",
    "\n",
    "#Load the dataset\n",
    "complete_dataset = datasets.ImageFolder(root=path, transform=data_transforms) \n",
    "dataset_size = len(complete_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31a8d932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 19762\n",
      "Train size: 15809\n",
      "Validation size: 1976\n",
      "Test size: 1977\n"
     ]
    }
   ],
   "source": [
    "#We are going to do a 80-10-10 split of the dataset to start\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "print(f\"Dataset size: {dataset_size}\")\n",
    "print(f\"Train size: {train_size}\")\n",
    "print(f\"Validation size: {val_size}\")\n",
    "print(f\"Test size: {test_size}\")\n",
    "\n",
    "#Using random_split to split the dataset into train, validation and test sets\n",
    "train_dataset, val_dataset, test_dataset = th.utils.data.random_split(complete_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bde1f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dataloaders\n",
    "batch_size = 16 #Update this if memory permits\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Manually defining the class names as per the dataset\n",
    "class_names = ['Metal', 'Glass', 'Biological', 'Paper', 'Battery', 'Trash', 'Cardboard', 'Shoes', 'Clothes', 'Plastic']\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e0653c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True) #Using ResNet50 as the base model for transfer learning\n",
    "\n",
    "#This takes the last layer of the model and replaces it with a new one\n",
    "num_ftrs = model.fc.in_features \n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "model = model.to(divice) #Moving the model to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a51762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#Learning rate is set to 1e-4 as a starting point, update this if needed\n",
    "learning_rate = 1.e-4 \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7688f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_empochs = 2 #Update this if needed\n",
    "train_accuracy_history = []\n",
    "val_accuracy_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15ddc319",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Backpropagation step\u001b[39;00m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m#Optimizer step\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#Get the stupid predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kyler\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kyler\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kyler\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Main training loop\n",
    "for epoch in  range(num_empochs):\n",
    "    model.train() #Sets the model to training mode\n",
    "    running_corrects = 0\n",
    "    running_samples = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        #Move the inputs and labels to the device\n",
    "        inputs = inputs.to(divice)\n",
    "        labels = labels.to(divice)\n",
    "\n",
    "        #Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() #Backpropagation step\n",
    "        optimizer.step() #Optimizer step\n",
    "\n",
    "        #Get the stupid predictions\n",
    "        preds = th.argmax(outputs, dim=1)\n",
    "        preds = preds.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        running_corrects += np.sum(preds == labels)\n",
    "        running_samples += labels.shape[0]\n",
    "    \n",
    "    #Calculate the accuracy\n",
    "    epoch_train_accuracy = running_corrects / running_samples\n",
    "    train_accuracy_history.append(epoch_train_accuracy)\n",
    "\n",
    "    #Validation step\n",
    "    model.eval() #Sets the model to evaluation mode\n",
    "    running_corrects = 0\n",
    "    running_samples = 0\n",
    "    with th.no_grad(): #Disables gradient calculation\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(divice)\n",
    "            labels = labels.to(divice)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds = th.argmax(outputs, dim=1)\n",
    "            preds = preds.cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            running_corrects += np.sum(preds == labels)\n",
    "            running_samples += labels.shape[0]\n",
    "\n",
    "    #Calculate the accuracy\n",
    "    epoch_val_accuracy = running_corrects / running_samples\n",
    "    val_accuracy_history.append(epoch_val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_empochs} - Train accuracy: {epoch_train_accuracy:.4f} - Validation accuracy: {epoch_val_accuracy:.4f}\")\n",
    "\n",
    "train_accuracy_history = np.array(train_accuracy_history)\n",
    "val_accuracy_history = np.array(val_accuracy_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
